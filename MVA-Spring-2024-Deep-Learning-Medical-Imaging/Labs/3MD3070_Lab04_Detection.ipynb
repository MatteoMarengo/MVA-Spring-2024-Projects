{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT1rKp85kKfO"
      },
      "source": [
        "# **TD on Detection of tumours in breast scans using Faster-RCNN**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCTsW4vN3i4Z"
      },
      "source": [
        "# **1. Install required APIs**\n",
        "\n",
        "Run the following cell to install required APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jvVBATllOqY"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.13.1  torchvision==0.14.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emWVwRPi-8k0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Install pycocotools\n",
        "git clone https://github.com/cocodataset/cocoapi.git\n",
        "cd cocoapi/PythonAPI\n",
        "python setup.py build_ext install\n",
        "cd ..\n",
        "cd ..\n",
        "\n",
        "# Install torchvision useful functions\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "\n",
        "cd ..\n",
        "\n",
        "pip install pycocotools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VavQFP2B3oz0"
      },
      "source": [
        "# **2. Import relevant packages**\n",
        "\n",
        "Run the following cell to install Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgRF8ZmC_v9h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import imageio.v2 as imageio\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSgyZU8z2oXq"
      },
      "source": [
        "# **3. The MIAS Database**\n",
        "\n",
        "All the practical exercises we will do in this notebook will use the publicly available  **Mammographic Image Analysis Society (MIAS) dataset**.\n",
        "\n",
        "- The MIAS dataset comprises **322** left and right mammograms.\n",
        "- The size of a mammogram is 1024x1024 pixels.\n",
        "- Each mammogram has various annotations (cf. the file Info.txt), for example:\n",
        "`mdb010 F CIRC B 525 425 33`\n",
        "                1st column: MIAS database reference number.\n",
        "\n",
        "                2nd column: Character of background tissue:\n",
        "                                F - Fatty\n",
        "                                G - Fatty-glandular\n",
        "                                D - Dense-glandular\n",
        "\n",
        "                3rd column: Class of abnormality present:\n",
        "                                CALC - Calcification\n",
        "                                CIRC - Well-defined/circumscribed masses\n",
        "                                SPIC - Spiculated masses\n",
        "                                MISC - Other, ill-defined masses\n",
        "                                ARCH - Architectural distortion\n",
        "                                ASYM - Asymmetry\n",
        "                                NORM - Normal\n",
        "\n",
        "                4th column: Severity of abnormality:\n",
        "                                B - Benign\n",
        "                                M - Malignant\n",
        "\n",
        "                5th,6th columns: x,y image-coordinates of centre of abnormality.\n",
        "\n",
        "                7th column: Approximate radius (in pixels) of a circle enclosing the abnormality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WKw1dkTVEwp"
      },
      "source": [
        "## 3.a. Download the MIAS Dataset\n",
        "\n",
        "Run the following cell to download the MIAS dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC0vx6anVEwu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Download the ddataset\n",
        "mkdir mias-db && cd mias-db\n",
        "wget http://peipa.essex.ac.uk/pix/mias/all-mias.tar.gz\n",
        "tar -zxvf all-mias.tar.gz\n",
        "rm all-mias.tar.gz && cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-OeJEh6giUe"
      },
      "source": [
        "## 3.b. Split dataset into train, validation and test sets.\n",
        "\n",
        "#### **You need to find the coordinates of square bounding box**.\n",
        "\n",
        "In the MIAS database, an abnormality's location in the mammogram is indicated by the pixel coordinates of its center $(x, y)$ and the radius $r$ in pixels of a circle enclosing the abnormality.\n",
        "Using $(x, y)$ and $r$, you can deduce the corresponding coordinates of a square bounding box enclosing the abnormality.\n",
        "\n",
        "- Complete the `get_square_bounding_box(file_info)` function to return the correct bounding box coordinates.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krEfU8BXVEw1"
      },
      "outputs": [],
      "source": [
        "# Path to database\n",
        "mias_db_path = './mias-db/'\n",
        "info_file = 'Info.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fY4rkZmVEw3"
      },
      "outputs": [],
      "source": [
        "def get_square_bounding_box(file_info):\n",
        "\n",
        "    if 'NORM' in file_info:\n",
        "        bbox = []\n",
        "    else:\n",
        "        x, y, r = int(file_info.split(' ')[4]), 1024-int(file_info.split(' ')[5]), int(file_info.split(' ')[6])\n",
        "        \"\"\"FILL HERE\"\"\"\n",
        "        bbox =\n",
        "    return bbox\n",
        "\n",
        "\n",
        "def read_dataset_info(mias_db_path, info_file, accepted_format=None, to_exclude=[]):\n",
        "    with open(os.path.join(mias_db_path, info_file), 'r') as fp:\n",
        "        if accepted_format is not None:\n",
        "            info = [f.strip() for f in fp.readlines() if f.startswith(accepted_format) and not f.startswith(to_exclude)]\n",
        "        else:\n",
        "            info = [f.strip() for f in fp.readlines() if f.startswith('mdb') and not f.startswith(to_exclude)]\n",
        "\n",
        "    dataset_info = {}\n",
        "    for file_info in info:\n",
        "        img_path = os.path.join(mias_db_path, file_info.split(' ')[0] + '.pgm')\n",
        "        class_name = \"NOTUMOUR\" if 'NORM' in file_info.split(' ')[2] else \"TUMOUR\"\n",
        "        bbox = get_square_bounding_box(file_info)\n",
        "\n",
        "        dataset_info[img_path] = {\n",
        "                                   \"class_name\": class_name,\n",
        "                                   \"bbox\": bbox}\n",
        "    return dataset_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAqjPA6NVEw5"
      },
      "outputs": [],
      "source": [
        "# Some cases contain abnormalities so we will exclude them.\n",
        "to_exclude = ('mdb216', 'mdb233', 'mdb245', 'mdb059')\n",
        "# Images to include in the validation set\n",
        "val_set = ('mdb001', 'mdb002', 'mdb005', 'mdb010', 'mdb012', 'mdb013')\n",
        "# Images to include in the test set\n",
        "test_set = ('mdb090', 'mdb091', 'mdb121', 'mdb134', 'mdb145', 'mdb218')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDjWt3hzVEw7"
      },
      "outputs": [],
      "source": [
        "train_dataset = read_dataset_info(mias_db_path, info_file, accepted_format=None, to_exclude=to_exclude + val_set + test_set)\n",
        "validation_dataset = read_dataset_info(mias_db_path, info_file, accepted_format=val_set, to_exclude=to_exclude)\n",
        "test_dataset = read_dataset_info(mias_db_path, info_file, accepted_format=test_set, to_exclude=to_exclude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_IFTrLPVEw9"
      },
      "outputs": [],
      "source": [
        "for img_path in test_dataset:\n",
        "\n",
        "    image = imageio.imread(img_path)\n",
        "    class_name = test_dataset[img_path]['class_name']\n",
        "    x1, y1, x2, y2  = test_dataset[img_path][\"bbox\"]\n",
        "\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], 'b-')\n",
        "    plt.axis('off')\n",
        "    plt.title(class_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfSr3u3xfoIg"
      },
      "source": [
        "## 3.c. Create your custom dataset class.\n",
        "\n",
        "To create a dataset class, you need to build on PyTorch's `Dataset` class. The structure of a dataset class looks as follows:\n",
        "\n",
        "        class MyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "            def __init__(self, args):\n",
        "                ...\n",
        "\n",
        "            def __getitem__(self):\n",
        "                return pil_image, target\n",
        "\n",
        "            def __len(self)__:\n",
        "                return len(self.img_list)\n",
        "\n",
        "\n",
        "**i. To create your own class, you need to override several methods such as:**\n",
        "- the constructor `__init__(self, args, transforms)`: One important detail is that the constructor of the dataset should have the variable `transforms`. It should then be used in `__getitem__(self, image_id)` to apply data transformation.\n",
        "- the `__getitem__(self, image_id)` method. This function should output a PIL image and a `target` dictionary containing ground-truth information about that image. For exmple, the classification label, the bounding box coordinates ... Data augmentation transformation should occur in that function with the `transforms` argument.\n",
        "- the `__len__(self)` method that returns the length of the dataset.\n",
        "\n",
        "\n",
        "In your custom class, you should also implement:\n",
        "\n",
        "**ii. In the constructor:**\n",
        "- `self.img_dict`: a dictionary that maps the id `idx` of an image to relevant info about it (path of the image, bounding box coordinates, class). __Warning__: what happens when there is no tumour ? Create fake bounding boxes.\n",
        "\n",
        "        self.img_dict[idx] = {'path': ...,\n",
        "                              'bbox': [...],\n",
        "                              'class': ...}\n",
        "                             \n",
        "- `self.class_names`: a dictionary that maps the tumour names to labels read by the model.\n",
        "\n",
        "        self.class_names = {\"TUMOUR\": 1,\n",
        "                            \"NOTUMOUR\": 2}\n",
        "                            \n",
        "- `self.idx2class`: a dictionary that does the inverse operation of `self.class_names`.\n",
        "\n",
        "**iii. New methods:**\n",
        "- `add_random_bbox(self)`: this function goes through the dataset and assign random bounding boxes to mammogram that do not have tumours. By doing this, you will give examples of healthy tissues to the model you will train.\n",
        "- `load_image(self, img_idx)`: this function should return the RGB PIL image indicated by the given `img_idx`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qN2pB1BGUgo"
      },
      "outputs": [],
      "source": [
        "class CancerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split_dataset, transforms=None, name=\"train\"):\n",
        "        self.split_dataset = split_dataset\n",
        "        self.name = name\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.class_names = {\n",
        "            \"TUMOUR\": 1,\n",
        "            \"NOTUMOUR\": 2,\n",
        "        }\n",
        "\n",
        "        self.add_random_bbox()\n",
        "        self.idx2class = {self.class_names[name]: name for name in self.class_names}\n",
        "\n",
        "         \"\"\"\n",
        "        FILL HERE\n",
        "        Create the self.img_dict dictionary here.\n",
        "        \"\"\"\n",
        "        self.img_dict =\n",
        "\n",
        "    def __getitem__(self, img_idx):\n",
        "        \"\"\"Generate an image from the specs of the given image ID.\n",
        "        Typically this function loads the image from a file, but\n",
        "        in this case it generates the image on the fly.\n",
        "        \"\"\"\n",
        "        \"\"\"FILL HERE\n",
        "        Create the image, label and bbox variables using the implemented img_dict dictionary.\n",
        "        \"\"\"\n",
        "        image =\n",
        "        class_name =\n",
        "        bbox =\n",
        "\n",
        "        # Compute the area of the bounding box\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        area = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "        # Convert everything to tensor\n",
        "        img_idx = torch.tensor([img_idx])\n",
        "        bbox = torch.as_tensor([bbox], dtype=torch.float32)\n",
        "        class_name = torch.as_tensor([class_name], dtype=torch.int64)\n",
        "        area = torch.as_tensor([area], dtype=torch.float32)\n",
        "\n",
        "\n",
        "        # Use the COCO template for targets to be able to evaluate the model with COCO API\n",
        "        target = {\"boxes\": bbox,\n",
        "                  \"labels\": class_name,\n",
        "                  \"image_id\": img_idx,\n",
        "                  \"area\": area,\n",
        "                  \"iscrowd\": torch.as_tensor([0], dtype=torch.int64)}\n",
        "\n",
        "        # Important line! don't forget to add this\n",
        "        if self.transforms:\n",
        "            image, target = self.transforms(image, target)\n",
        "        # return the image, the boxlist and the idx in your dataset\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.split_dataset)\n",
        "\n",
        "\n",
        "\n",
        "    def add_random_bbox(self):\n",
        "        # Add a random bounding box for all images that are NORMAL.\n",
        "        for img_path in self.split_dataset:\n",
        "            if self.split_dataset[img_path]['class_name'] == \"NOTUMOUR\":\n",
        "                # where is there something in the mammogram ?\n",
        "                img = imageio.imread(img_path)\n",
        "                # Define random radius that should not be bigger than image\n",
        "                radius = np.random.randint(10, 70)\n",
        "                # Set borders at zero to avoid having a bounding box that is outside the mammogram\n",
        "                new_img = np.zeros(img.shape)\n",
        "                new_img[radius:-radius, radius:-radius] = img[radius:-radius, radius:-radius]\n",
        "                mask = new_img > 50\n",
        "                mask_id_x, mask_id_y = np.where(mask)\n",
        "                random_id_x, random_id_y = np.random.randint(len(mask_id_x)), np.random.randint(len(mask_id_y))\n",
        "                center_x, center_y = mask_id_x[random_id_x], mask_id_y[random_id_y]\n",
        "                bbox = [center_x-radius, center_y-radius, center_x+radius, center_y+radius]\n",
        "                self.split_dataset[img_path]['bbox'] = bbox\n",
        "\n",
        "\n",
        "\n",
        "    def load_image(self, img_idx):\n",
        "        \"\"\"Generate an image from the specs of the given image ID.\n",
        "        Typically this function loads the image from a file, but\n",
        "        in this case it generates the image on the fly.\n",
        "        \"\"\"\n",
        "        img_path = self.img_dict[img_idx]['path']\n",
        "        image = imageio.imread(img_path)[..., np.newaxis]\n",
        "        image = np.concatenate((image, image, image), axis=2)\n",
        "        image = Image.fromarray(image).convert(\"RGB\")\n",
        "        return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A_qrOJVhY0P"
      },
      "source": [
        "## 3.d. Data augmentation\n",
        "\n",
        "The `get_transform(train)` function applies random flips to the images during training.\n",
        "\n",
        "In order to flip an image as well as the corresponding bounding box coordinates, a `RandomHorizontalFlip(object)` class was implemented.\n",
        "\n",
        "If you wish, you can implement your own data augmentation class to perform vertical flips or rotations.\n",
        "\n",
        "    class RandomHorizontalFlip(object):\n",
        "\n",
        "        def __init__(self, prob):\n",
        "            self.prob = prob\n",
        "\n",
        "        def __call__(self, image, target):\n",
        "            if random.random() < self.prob:\n",
        "                height, width = image.shape[-2:]\n",
        "                # Flip image\n",
        "                image = image.flip(-1)\n",
        "                # Flip bounding box coordinates\n",
        "                bbox = target[\"boxes\"]\n",
        "                bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "                target[\"boxes\"] = bbox\n",
        "            return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtjdUdBnrRxC"
      },
      "outputs": [],
      "source": [
        "import transforms as T\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hi16zOkhgxH"
      },
      "source": [
        "## 3.e. Instantiate train, validation and test sets\n",
        "\n",
        "- Create your train, validation and test sets using the `CancerDataset` class.\n",
        "- How imbalanced is the training dataset ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxNv-2ru1p2M"
      },
      "outputs": [],
      "source": [
        "\"\"\" FILL HERE\n",
        "create your train val and test datasets\n",
        "\"\"\"\n",
        "train =\n",
        "val =\n",
        "test =\n",
        "\n",
        "print(\"Number of images in training set: {}\".format(len(train)))\n",
        "print(\"Number of images in validation set: {}\".format(len(val)))\n",
        "print(\"Number of images in test set: {}\".format(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSSifbn2VExg"
      },
      "outputs": [],
      "source": [
        "# How imbalanced is the training set ?\n",
        "class_counts = {\"TUMOUR\": 0, \"NOTUMOUR\": 0}\n",
        "for img_idx in train.img_dict:\n",
        "    class_name = train.idx2class[train.img_dict[img_idx]['class']]\n",
        "    if \"NO\" in class_name: class_counts['NOTUMOUR'] += 1\n",
        "    else: class_counts['TUMOUR'] += 1\n",
        "\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26CJAXSD4tt1"
      },
      "source": [
        "## 3.f. Visualize some images with their bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcYC2DS8VExi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for img_idx in range(20, 25):\n",
        "\n",
        "    img_path = train.img_dict[img_idx]['path']\n",
        "    image = imageio.imread(img_path)\n",
        "    class_name = train.idx2class[train.img_dict[img_idx]['class']]\n",
        "    x1, y1, x2, y2  = train.img_dict[img_idx][\"bbox\"]\n",
        "\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    if \"NOTUMOUR\" in class_name:\n",
        "        plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], 'g-')\n",
        "    else:\n",
        "        plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], 'r-')\n",
        "    plt.axis('off')\n",
        "    plt.title(class_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKZ19CQq4zQp"
      },
      "source": [
        "## 3.g. Create the data loaders\n",
        "\n",
        "\n",
        "In this section, we instantiate **data loaders** that will be used to generate batches of images on the fly during training.\n",
        "\n",
        "For each of the created datasets, you need to call `torch.utils.data.DataLoader` and define :\n",
        "- the `batch_size`,\n",
        "- whether to randomly shuffle the dataset so the dataloaders will return random samples by setting the `shuffle` parameter to `True` or `False`. Typically, you want to shuffle your training dataset. It does not matter for the validation and test sets.\n",
        "- the number of processes that should be used to load each batch using `num_workers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NW-LYe4gGbO"
      },
      "outputs": [],
      "source": [
        "# Data loaders\n",
        "# torch.manual_seed(1)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train, batch_size=8, shuffle=True, num_workers=1,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(\n",
        "    val, batch_size=1, shuffle=False, num_workers=1,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test, batch_size=1, shuffle=False, num_workers=1,\n",
        "    collate_fn=utils.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAYljfrmzMvx"
      },
      "source": [
        "# **4. Faster Region-based Convolutional Network model - [code](https://github.com/pytorch/vision/blob/master/torchvision/models/detection/faster_rcnn.py), [article](https://arxiv.org/abs/1506.01497)**\n",
        "\n",
        "The **FasterRCNN** architecture consists of the RPN as a region proposal algorithm and the Fast RCNN as a detector network.\n",
        "\n",
        "For this practical exercise we will train a **Faster R-CNN model with a ResNet-50-FPN backbone** instead of VGG.\n",
        "\n",
        "![](https://bit.ly/3BoJCjj)\n",
        "\n",
        ">  The input to the model is expected to be a list of tensors (one per image), each of shape `[C, H, W]`  (color channels first) and should be in 0-1 range. Different images can have different sizes.\n",
        "\n",
        "> During **training**, the model expects both the input tensors and targets (list of dictionary). Each `target` dictionary contains:\n",
        "> - `boxes` (`FloatTensor` of size `[N, 4]`): the ground-truth bounding boxes in [x1, y1, x2, y2] format, with `0 <= x1 < x2 <= W` and `0 <= y1 < y2 <= H.`\n",
        "> - `labels` (`Int64Tensor` of size `[N]`): the class label for each ground-truth box\n",
        ">- `image_id` (`Int64Tensor` of size `[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
        ">- `area` (`Tensor` of size `[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
        "\n",
        "> The model returns a `Dict[Tensor]` during training, containing the classification and regression losses for both the RPN and the R-CNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRbXJcYQVExn"
      },
      "source": [
        "## Exhaustive list of Faster RCNN's input arguments\n",
        "\n",
        "**About the architecture:**\n",
        "> - `backbone (nn.Module)`: the network used to compute the features for the model.\n",
        "    It should contain a out_channels attribute, which indicates the number of output\n",
        "    channels that each feature map has (and it should be the same for all feature maps).\n",
        "    The backbone should return a single `Tensor` or an `OrderedDict[Tensor]`.\n",
        "    \n",
        "**About the input data:**\n",
        "> Classes:\n",
        "> - `num_classes (int)`: number of output classes of the model (including the background).\n",
        "    If box_predictor is specified, `num_classes` should be None.\n",
        "\n",
        "> Image size rescaling:\n",
        "> - `min_size (int)`: minimum size of the image to be rescaled before feeding it to the backbone\n",
        "> - `max_size (int)`: maximum size of the image to be rescaled before feeding it to the backbone\n",
        "\n",
        "> Image normalization:\n",
        "> - `image_mean (Tuple[float, float, float])`: mean values used for input normalization.\n",
        "    They are generally the mean values of the dataset on which the backbone has been trained\n",
        "    on\n",
        "> - `image_std (Tuple[float, float, float])`: std values used for input normalization.\n",
        "    They are generally the std values of the dataset on which the backbone has been trained on\n",
        "    \n",
        "**About the Region Proposal Network (RPN):**\n",
        "\n",
        "> Architecture:\n",
        ">- `rpn_anchor_generator (AnchorGenerator)`: module that generates the anchors for a set of feature\n",
        "    maps.\n",
        ">- `rpn_head (nn.Module)`: module that computes the objectness and regression deltas from the RPN\n",
        "\n",
        "> NMS parameters:\n",
        "> - `rpn_pre_nms_top_n_train (int)`: number of proposals to keep before applying NMS during training\n",
        "> - `rpn_pre_nms_top_n_test (int)`: number of proposals to keep before applying NMS during testing\n",
        "> - `rpn_post_nms_top_n_train (int)`: number of proposals to keep after applying NMS during training\n",
        "> - `rpn_post_nms_top_n_test (int)`: number of proposals to keep after applying NMS during testing\n",
        "> - `rpn_nms_thresh (float)`: NMS threshold used for postprocessing the RPN proposals\n",
        "\n",
        "> IoU thresholds:\n",
        ">- `rpn_fg_iou_thresh (float)`: minimum IoU between the anchor and the GT box so that they can be\n",
        "    considered as positive during training of the RPN.\n",
        ">- `rpn_bg_iou_thresh (float)`: maximum IoU between the anchor and the GT box so that they can be\n",
        "    considered as negative during training of the RPN.\n",
        "\n",
        "> RPN parameters for training:\n",
        ">- `rpn_batch_size_per_image (int)`: number of anchors that are sampled during training of the RPN\n",
        "    for computing the loss\n",
        ">- `rpn_positive_fraction (float)`: proportion of positive anchors in a mini-batch during training\n",
        "    of the RPN\n",
        "\n",
        "> RPN parameter for inference:\n",
        "> - `rpn_score_thresh (float)`: during inference, only return proposals with a classification score\n",
        "    greater than `rpn_score_thresh`\n",
        "    \n",
        "**About bounding box processing and proposals:**\n",
        "\n",
        "> Architecture:\n",
        ">- `box_roi_pool (MultiScaleRoIAlign)`: the module which crops and resizes the feature maps in\n",
        "    the locations indicated by the bounding boxes\n",
        ">- `box_head (nn.Module)`: module that takes the cropped feature maps as input\n",
        ">- `box_predictor (nn.Module)`: module that takes the output of box_head and returns the\n",
        "    classification logits and box regression deltas.\n",
        "\n",
        "> Inference:\n",
        ">- `box_score_thresh (float)`: during inference, only return proposals with a classification score\n",
        "    greater than `box_score_thresh`\n",
        "> - `box_nms_thresh (float)`: NMS threshold for the prediction head. Used during inference\n",
        "> - `box_detections_per_img (int)`: maximum number of detections per image, for all classes.\n",
        "\n",
        "> Training:\n",
        ">- `box_fg_iou_thresh (float)`: minimum IoU between the proposals and the GT box so that they can be\n",
        "    considered as positive during training of the classification head\n",
        ">- `box_bg_iou_thresh (float)`: maximum IoU between the proposals and the GT box so that they can be\n",
        "    considered as negative during training of the classification head\n",
        ">- `box_batch_size_per_image (int)`: number of proposals that are sampled during training of the\n",
        "    classification head\n",
        ">- `box_positive_fraction (float)`: proportion of positive proposals in a mini-batch during training\n",
        "    of the classification head\n",
        ">- `bbox_reg_weights (Tuple[float, float, float, float])`: weights for the encoding/decoding of the\n",
        "    bounding boxes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pn1RrTbVExo"
      },
      "source": [
        "## 4.a. Data normalization\n",
        "\n",
        "Here you will compute the values of the `image_mean` and `image_std` arguments. The outputs of the functions should be **tuples**, one element (mean and std) per color channel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLE1Vsj8r5Vl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_means(dataset):\n",
        "    \"\"\"\n",
        "    FILL HERE\n",
        "    \"\"\"\n",
        "    means =\n",
        "    return tuple(means)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl2iHaA3s3bn"
      },
      "outputs": [],
      "source": [
        "def compute_stds(dataset):\n",
        "    \"\"\"\n",
        "    FILL HERE\n",
        "    \"\"\"\n",
        "    stds =\n",
        "    return tuple(stds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F6TzeNHs8vF"
      },
      "outputs": [],
      "source": [
        "image_mean = compute_means(train)\n",
        "image_std = compute_stds(train)\n",
        "\n",
        "\n",
        "print(\"Means: {}\".format(image_mean))\n",
        "print(\"Stds: {}\".format(image_std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4GGLeRrVExs"
      },
      "source": [
        "## 4.b. Instantiate the model.\n",
        "\n",
        "To instantiate the model, you need to:\n",
        "- call `torchvision.models.detection.fasterrcnn_resnet50_fpn` and give it the mean and standard deviation of the training set.\n",
        "- indicate the number of classes of our problems: `NOTUMOUR`, `TUMOUR` and one for the background of an image, so 3 classes.\n",
        "- replace the box predictor head by FastRCNN `FastRCNNPredictor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qcomEfZr2Qe"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load a model pre-trained pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True,\n",
        "                                                             image_mean=image_mean,\n",
        "                                                             image_std=image_std)\n",
        "\n",
        "# replace the classifier with a new one, that has num_classes which is user-defined\n",
        "num_classes = 3  # intumomur + no tumour + background\n",
        "\n",
        "# get number of input channels for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkSNvqkjSHYu"
      },
      "source": [
        "## 4.c. Compute the number of trainable parameters in the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPsCQAxk-7yO"
      },
      "outputs": [],
      "source": [
        "\"\"\" FILL HERE \"\"\"\n",
        "params =\n",
        "print(\"Number of trainable parameters: {:.4e}\".format(params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIJMa4O5zJNS"
      },
      "source": [
        "# **5. Training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCMGV-XDXpJd"
      },
      "source": [
        "## 5.a. Optimizer and Hyperparameters\n",
        "\n",
        "> Define the optimizer and the associated hyperparameters to use for training:\n",
        ">- initial learning rate,\n",
        ">- momentum,\n",
        ">- weight decay,\n",
        ">- ...\n",
        "\n",
        "> Use a learning rate scheduler to slowly decrease the learning rate during the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uapHMOU3dmO3"
      },
      "outputs": [],
      "source": [
        "# move model to the right device\n",
        "model.cuda()\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "\"\"\" FILL HERE\"\"\"\n",
        "optimizer =\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate\n",
        "# Change the scheduler type if you wish\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=10,\n",
        "                                               gamma=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frGCNluVT-ct",
        "tags": []
      },
      "source": [
        "## 5.b. Training and validation functions for on epoch.\n",
        "\n",
        ">  Implement your own training function for one epoch. The function should:\n",
        ">- Go over the dataloader and fetch batches\n",
        ">- Run the model's prediction,\n",
        ">- Compute the loss,\n",
        ">- Re-initialize the optimizer: for every mini-batch during the training phase, you need to explicitly set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. If you don't do that, you risk an `Out-Of-Memory` error.\n",
        ">- Do the backpropagation  (compute the gradients for every trainable parameter in the model).\n",
        ">- Optimizer update (update the model's weights and biases using the computed gradients).\n",
        "\n",
        "> Implement a similar function meant to compute the losses over the validation set. In this function, you do not need to compute the gradients or initialize and update the optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6Jjniq7VExz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip3 install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiJLUg7uVEx0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcK8Gisnljfe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, epoch, writer):\n",
        "\n",
        "    # Set the model in training mode: the gradients will be saved.\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = {}\n",
        "    for i, values in enumerate(data_loader):\n",
        "        images, targets = values\n",
        "\n",
        "        # Create list of input images\n",
        "        images = list(image.cuda() for image in images)\n",
        "        # Create list of ground-truth dicionnaries\n",
        "        targets = [{k: v.cuda() for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Feed the training samples to the model\n",
        "        # The model returns loss_dict which contains the values of every loss functions\n",
        "        loss_dict = model(images, targets)\n",
        "        # Compute the global loss by summing all loss values\n",
        "        global_loss = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = global_loss.item()\n",
        "\n",
        "        # Increment the epoch's loss\n",
        "        for k, v in loss_dict.items():\n",
        "            epoch_loss[k] = epoch_loss.get(k, []) + [v.item()]\n",
        "        epoch_loss['global_loss'] = epoch_loss.get('global_loss', []) + [loss_value]\n",
        "\n",
        "        # If your loss is a Nan or infinite, you need to stop the training because it's failing.\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict)\n",
        "            sys.exit(1)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # Backpropagation: compute gradients\n",
        "        global_loss.backward()\n",
        "        # Update the model's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    # Compute the losses over the whole epoch\n",
        "    for k, v in epoch_loss.items():\n",
        "        epoch_loss[k] = np.mean(v)\n",
        "        writer.add_scalar('Training Loss/{}'.format(k), np.mean(v), epoch)\n",
        "    writer.flush()\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moortphv6zix",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, data_loader, epoch, writer):\n",
        "\n",
        "    validation_loss = {}\n",
        "    for i, values in enumerate(data_loader):\n",
        "        images, targets = values\n",
        "        images = list(image.cuda() for image in images)\n",
        "        targets = [{k: v.cuda() for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        global_loss = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = global_loss.item()\n",
        "\n",
        "        # Increment the epoch's loss\n",
        "        for k, v in loss_dict.items():\n",
        "            validation_loss[k] = validation_loss.get(k, []) + [v.item()]\n",
        "        validation_loss['global_loss'] = validation_loss.get('global_loss', []) + [loss_value]\n",
        "\n",
        "    # Compute the losses over the whole epoch\n",
        "    for k, v in validation_loss.items():\n",
        "        validation_loss[k] = np.mean(v)\n",
        "        writer.add_scalar('Validation Loss/{}'.format(k), np.mean(v), epoch)\n",
        "    writer.flush()\n",
        "    return validation_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMc5tV1Vc2W"
      },
      "source": [
        "## 5.c. Train your model\n",
        "\n",
        "Define the number of epochs `num_epochs` over which you wish to train your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxYmwb1Ydout"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "    # Train for one epoch, printing every 10 iterations\n",
        "    start = time.time()\n",
        "    epoch_loss = train_one_epoch(model, optimizer, train_data_loader, epoch, writer)\n",
        "    result = \"Epoch {} [{:.1f} s] - lr: {:.3e}:\".format(epoch, time.time()-start, lr_scheduler.get_last_lr()[0])\n",
        "    for k, v in epoch_loss.items(): result += \"\\t{}: {:.6f}\".format(k, v)\n",
        "    print(result)\n",
        "\n",
        "    # Compute losses over the validation set\n",
        "    validation_loss = validate_one_epoch(model, val_data_loader, epoch, writer)\n",
        "    result = \"Validation:\"\n",
        "    for k, v in validation_loss.items(): result += \"\\t{}: {:.6f}\".format(k, v)\n",
        "    print(result)\n",
        "\n",
        "\n",
        "    # Update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Save the model if you wish. You can add a criteria before saving, for example\n",
        "    # if the validation decreases.\n",
        "    # save_path = \"/content/drive/My Drive/mva_td/saved_models/my_model\"\n",
        "    # torch.save(model, save_path)\n",
        "\n",
        "#     torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuKHiz112AEC"
      },
      "source": [
        "## 5.d. Learning curves with Tensorboard\n",
        "\n",
        "Run the following cell to plot the learning curves in Tensorboard.\n",
        "\n",
        "Is your model overfitting ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hu91s3naPWF"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir './runs/'\n",
        "\n",
        "from tensorboard import notebook\n",
        "notebook.list() # View open TensorBoard instances\n",
        "notebook.display(port=6006, height=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO7numj-VEx6"
      },
      "outputs": [],
      "source": [
        "!tensorboard --logdir=runs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-HSSRTyzDNL"
      },
      "source": [
        "# **6. Evaluation over the test set**\n",
        "\n",
        "> During **inference**, the model requires only the list of input tensors, and returns the post-processed predictions as a `List[Dict[Tensor]]`, one for each input image. The fields of the Dict are as follows, where N is the number of detections:\n",
        "> - `boxes` (`FloatTensor` of size `[N, 4]`): the predicted boxes in `[x1, y1, x2, y2]` format, with `0 <= x1 < x2 <= W` and `0 <= y1 < y2 <= H`.\n",
        "> - `labels` (`Int64Tensor` of size `[N]`): the predicted labels for each detection\n",
        "> - `scores` (`Tensor` of size `[N]`): the scores of each detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeJMJpkQWrrV"
      },
      "source": [
        "## 6.a. Visualisation of the predictions\n",
        "\n",
        "\n",
        "In evaluation mode, the model does not have access to the ground-truth `targets`. You have to set it in evaluation mode using `model.eval()`. That way, the model takes as input only the list of input tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-CsgHXdVEx8"
      },
      "outputs": [],
      "source": [
        "def visualize(image, target, prediction):\n",
        "\n",
        "\n",
        "    gt_bbox = target['boxes'][0].cpu().numpy()\n",
        "    gt_label = target['labels'].cpu().numpy()\n",
        "\n",
        "    pred_bboxs = prediction['boxes'].cpu().detach().numpy()\n",
        "    pred_labels = prediction['labels'].cpu().detach().numpy()\n",
        "    pred_scores = prediction['scores'].cpu().detach().numpy()\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    # Plot mammogram\n",
        "    image = image.mul(255).permute(1, 2, 0).cpu().byte()\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot ground-truth bounding box\n",
        "    x1, y1, x2, y2 = gt_bbox\n",
        "    line1, = plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], 'b-', label='Ground-truth')\n",
        "\n",
        "    # Plot predicted bounding boxes\n",
        "    for i in range(len(pred_labels)):\n",
        "        x1, y1, x2, y2 = pred_bboxs[i]\n",
        "        score = pred_scores[i]\n",
        "\n",
        "        if pred_labels[i] == 1:\n",
        "            c = 'r'\n",
        "            label='Prediction: tumour'\n",
        "            plt.annotate('{:.1f}'.format(100*score), (x1, y1), c=c, fontsize='medium')\n",
        "        else:\n",
        "            c = 'g'\n",
        "            label='Prediction: no tumour'\n",
        "            plt.annotate('{:.1f}'.format(100*score), (x2, y2), c=c, fontsize='medium')\n",
        "        line, = plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], c, label=label)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcISZFIdVEx9"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "for i, values in enumerate(test_data_loader):\n",
        "        images, targets = values\n",
        "\n",
        "        # Create list of input images\n",
        "        images = list(image.cuda() for image in images)\n",
        "        # Create list of ground-truth dicionnaries\n",
        "        targets = [{k: v.cuda() for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Feed the training samples to the model\n",
        "        # The model returns  the predictions\n",
        "        predictions = model(images)\n",
        "\n",
        "        visualize(images[0], targets[0], predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSZ7MyBYy4VJ"
      },
      "source": [
        "## 6.b. Evaluation\n",
        "\n",
        "Use the COCO evaluation function to compute metrics such as mAP over the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jaMaBEhzRkQ"
      },
      "outputs": [],
      "source": [
        "from engine import evaluate\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluate(model, test_data_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-o1hLgE65DS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "0.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}