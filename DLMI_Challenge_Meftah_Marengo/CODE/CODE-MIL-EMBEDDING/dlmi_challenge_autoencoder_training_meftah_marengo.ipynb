{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7931873,
          "sourceType": "datasetVersion",
          "datasetId": 4451627
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "----\n",
        "# <b> DLMI Challenge </b>\n",
        "# <b> Autoencoders training </b>\n",
        "# <b> Matteo MARENGO | matteo.marengo@ens-paris-saclay.fr </b>\n",
        "# <b> Manal MEFTAH | manal.meftah@ens-paris-saclay.fr </b>"
      ],
      "metadata": {
        "id": "sIlFwlMH7pg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "----\n",
        "# <b> Import libraries </b>"
      ],
      "metadata": {
        "id": "qBjGH_P27pg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T19:27:18.334768Z",
          "iopub.execute_input": "2024-03-28T19:27:18.335128Z",
          "iopub.status.idle": "2024-03-28T19:27:24.840031Z",
          "shell.execute_reply.started": "2024-03-28T19:27:18.335098Z",
          "shell.execute_reply": "2024-03-28T19:27:24.839199Z"
        },
        "trusted": true,
        "id": "1Pv7TfgT7pg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "----\n",
        "# <b> Load the model </b>"
      ],
      "metadata": {
        "id": "J0kYjH8P7pg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CombinedDataset(Dataset):\n",
        "    def __init__(self, folders, transform=None):\n",
        "        self.folders = folders\n",
        "        self.transform = transform\n",
        "        self.file_list = []\n",
        "        for folder_path in folders:\n",
        "            self.file_list.extend([\n",
        "                os.path.join(dp, f) for dp, dn, filenames in os.walk(folder_path) for f in filenames if os.path.splitext(f)[1].lower() in ['.png', '.jpg', '.jpeg']\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, image  # Autoencoder input and output are the same\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "combined_dataset = CombinedDataset(['/kaggle/input/dlmi-mms-data/dlmi-lymphocytosis-classification/trainset', '/kaggle/input/dlmi-mms-data/dlmi-lymphocytosis-classification/testset'], transform=transform)\n",
        "combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, 112, 112)\n",
        "            nn.ReLU(True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),  # Output: (64, 56, 56)\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Output: (128, 28, 28)\n",
        "            nn.ReLU(True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),  # Output: (128, 14, 14)\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Output: (256, 7, 7)\n",
        "            nn.ReLU(True),\n",
        "            nn.Flatten(),  # Flatten the output for the fully connected layer\n",
        "            nn.Linear(256*7*7, 4096),  # Compress to latent space\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(4096, 256*7*7),  # Expand from latent space\n",
        "            nn.ReLU(True),\n",
        "            nn.Unflatten(1, (256, 7, 7)),  # Unflatten to get back to the convolutional tensor shape\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Output: (128, 14, 14)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, 28, 28)\n",
        "            nn.ReLU(True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),  # Output: (64, 56, 56)\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),  # Output: (3, 112, 112)\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),  # Output: (3, 224, 224)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-28T19:27:24.841475Z",
          "iopub.execute_input": "2024-03-28T19:27:24.842083Z",
          "iopub.status.idle": "2024-03-28T19:27:32.041348Z",
          "shell.execute_reply.started": "2024-03-28T19:27:24.842058Z",
          "shell.execute_reply": "2024-03-28T19:27:32.040529Z"
        },
        "trusted": true,
        "id": "AeSHmNMG7pg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "----\n",
        "# <b> Do the training </b>"
      ],
      "metadata": {
        "id": "eOHLRKXI7pg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, loss function, and optimizer setup\n",
        "model = Autoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model with tqdm progress bar\n",
        "epoch_losses = []\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    n_batches = 0\n",
        "    with tqdm(total=len(combined_loader), desc=f'Epoch {epoch+1}/{epochs}', position=0, leave=True) as pbar:\n",
        "        for data in combined_loader:\n",
        "            imgs, _ = data\n",
        "            imgs = imgs.to(device)  # Move inputs to the device\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, imgs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            n_batches += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({'Loss': loss.item()})\n",
        "    average_epoch_loss = epoch_loss / n_batches\n",
        "    epoch_losses.append(average_epoch_loss)\n",
        "    print(f'Epoch {epoch+1}, Average Loss: {average_epoch_loss}')\n",
        "\n",
        "# After training, plot the loss evolution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epoch_losses, label='Training Loss')\n",
        "plt.title('Loss Evolution')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('/kaggle/working/loss_evolution.png')\n",
        "plt.show()\n",
        "\n",
        "# Save the encoder weights\n",
        "torch.save(model.encoder.state_dict(), '/kaggle/working/encoder_weights_full_images.pth')\n",
        "\n",
        "# Save the decoder weights\n",
        "torch.save(model.decoder.state_dict(), '/kaggle/working/decoder_weights_full_images.pth')\n",
        "\n",
        "print(\"Model trained and encoder weights saved!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "bkw5mnLN7pg8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}